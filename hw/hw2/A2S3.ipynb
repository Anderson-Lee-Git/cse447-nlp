{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLDBRhYnS6Pu"
      },
      "source": [
        "# Instructions\n",
        "- Before running the jupyter notebook, don't forget to copy it into your drive **(`File` => `Save a copy in Drive`)**. *Failing to do this step may result in losing the progress of your code.*\n",
        "- Change your resource type to GPU before progressing **(`Runtime` => `Change runtime time` => `T4 GPU`).**\n",
        "- There are six steps for this exercise (including step 0, the preperation step). **You will do the following two tasks as detailed under each step, and we will grade both parts:**\n",
        "  - **Coding Exercises:** You will complete the the code blocks denoted by **`TODO:`**.\n",
        "  - **Questions to Answer:** You will answer questions denoted by **`Q:`**.\n",
        "- For the submission of the assignment, please download this notebook as a **Python file**, named `A2S3.py`.\n",
        "\n",
        "# Step 0: Preperation\n",
        "\n",
        "**Step 0.1:** Install dependency\n",
        "- This step could take a while.\n",
        "\n",
        "**Step 0.2:** Mount data and files to your drive.\n",
        "- You will see a few popup windows asking for your authorization for this notebook to access your Google Drive files. You need to say yes to all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKtzKEQ_SCyw",
        "outputId": "b0f544fb-0b66-4726-d252-4b4d98e0dfa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (2.2.0)\n",
            "Requirement already satisfied: transformers in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (4.37.2)\n",
            "Requirement already satisfied: datasets in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (2.17.0)\n",
            "Requirement already satisfied: tqdm in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (4.66.1)\n",
            "Requirement already satisfied: gdown==v4.6.3 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (4.6.3)\n",
            "Requirement already satisfied: filelock in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from gdown==v4.6.3) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from gdown==v4.6.3) (2.31.0)\n",
            "Requirement already satisfied: six in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from gdown==v4.6.3) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from gdown==v4.6.3) (4.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
            "Requirement already satisfied: xxhash in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from requests[socks]->gdown==v4.6.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from requests[socks]->gdown==v4.6.3) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from requests[socks]->gdown==v4.6.3) (2.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from requests[socks]->gdown==v4.6.3) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from beautifulsoup4->gdown==v4.6.3) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from requests[socks]->gdown==v4.6.3) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/anderson/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "mkdir: checkpoints: File exists\n"
          ]
        }
      ],
      "source": [
        "# Step 0.1: Install dependency and download codebase\n",
        "!pip install torch transformers datasets tqdm gdown==v4.6.3\n",
        "!mkdir checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq_4e2P7FWg9"
      },
      "source": [
        "# Step 1: Defining PyTorch Dataset and Dataloader\n",
        "\n",
        "First, you will implement a dataset class (named `SST2Dataset`) for processing the SST-2 dataset. You can find details of the basics of Dataset and Dateloader in this [tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
        "\n",
        "We defined the `SST2Example` class for you, which is used to convert a dict of raw data into an SST2Example object that contains a text and label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BZSVUen2Fut9"
      },
      "outputs": [],
      "source": [
        "# Load necessary packages\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import PreTrainedTokenizerFast, AutoTokenizer, RobertaForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "\n",
        "######################################################\n",
        "#  The following code is given to you.\n",
        "######################################################\n",
        "\n",
        "@dataclass\n",
        "class SST2Example:\n",
        "    \"\"\"\n",
        "    Convert a dict of raw data into an SST2Example object that contains a text and label.\n",
        "    If you're interested, you can find descriptions of dataclass at https://docs.python.org/3/library/dataclasses.html\n",
        "    \"\"\"\n",
        "    text: str\n",
        "    label: int  # 0 for negative, 1 for positive\n",
        "\n",
        "    @staticmethod\n",
        "    def from_dict(data: dict):\n",
        "        text = data['text']\n",
        "        label = data['label']\n",
        "\n",
        "        return SST2Example(\n",
        "            text=text,\n",
        "            label=label,\n",
        "        )\n",
        "\n",
        "\n",
        "def initialize_datasets(tokenizer: PreTrainedTokenizerFast) -> dict:\n",
        "    \"\"\"\n",
        "    Initialize the dataset objects for all splits based on the raw data.\n",
        "    :param tokenizer: A tokenizer used to prepare the inputs for a model (see details in https://huggingface.co/docs/transformers/main_classes/tokenizer).\n",
        "    :return: A dictionary of the dataset splits.\n",
        "    \"\"\"\n",
        "    raw_data = load_dataset(\"gpt3mix/sst2\")\n",
        "    split_datasets = {}\n",
        "\n",
        "    for split_name in raw_data.keys():\n",
        "        split_data = list(raw_data[split_name])\n",
        "\n",
        "        split_datasets[split_name] = SST2Dataset(tokenizer, split_data)\n",
        "\n",
        "    return split_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LX8SLHbSyzv"
      },
      "source": [
        "## **Coding Exercises** for Step 1:\n",
        "Below, we provide a skeleton for creating a SST-3 Dataset object. **You will complete the following code blocks denoted by `TODO:`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lCfmeRwBK1rD"
      },
      "outputs": [],
      "source": [
        "class SST2Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Create a customized dataset object for SST-2.\n",
        "    A custom Dataset class must implement three functions: __init__, __len__, and __getitem__.\n",
        "    You can find a detailed tutorial on Dataset at https://pytorch.org/tutorials/beginner/basics/data_tutorial.html.\n",
        "    \"\"\"\n",
        "    tokenizer: PreTrainedTokenizerFast = None\n",
        "\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizerFast, raw_data_list: List[dict]):\n",
        "        SST2Dataset.tokenizer = tokenizer\n",
        "        self.sample_list = [SST2Example.from_dict(data) for data in raw_data_list]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Get the number of items in the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: return the number of samples in sample_list.\n",
        "        return len(self.sample_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get the idx-th item from the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: return the idx-th item in sample_list.\n",
        "        return self.sample_list[idx]\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Get an iterator for the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: return an iterator for sample_list.\n",
        "        return iter(self.sample_list)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batched_samples: List[SST2Example]) -> dict:\n",
        "        \"\"\"\n",
        "        Encode samples in batched_samples: tokenize the input texts, and turn labels into a tensor.\n",
        "        :param batched_samples: A list of SST2Example samples.\n",
        "        :return: A dictionary of encoded texts and their corresponding labels (in tensors).\n",
        "        \"\"\"\n",
        "        # TODO: collect all input texts from batched_samples into a list.\n",
        "        batched_text = [sample.text for sample in batched_samples]\n",
        "\n",
        "        # TODO: collect all labels from batched_samples into a list.\n",
        "        batched_label = [sample.label for sample in batched_samples]\n",
        "\n",
        "        # Tokenize the input texts.\n",
        "        text_encoding = SST2Dataset.tokenizer(batched_text,\n",
        "                                              padding=True,\n",
        "                                              max_length=512,\n",
        "                                              truncation=True,\n",
        "                                              return_tensors=\"pt\")\n",
        "\n",
        "        # TODO: convert data type of the labels to torch.long (Hint: using torch.LongTensor).\n",
        "        label_encoding = torch.LongTensor(batched_label)\n",
        "\n",
        "        # TODO: return dictionary of encoded texts and labels.\n",
        "        return {\n",
        "            \"text_encoding\": text_encoding,\n",
        "            \"label_encoding\": label_encoding,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRL-3NzOcFOE"
      },
      "source": [
        "## **Questions to Answer** for Step 1:\n",
        "**Answer these questions in your write-up report.**\n",
        "- **Q1.1:** Explain the usages of the following arguments when you encode the input texts: `padding`, `max_length`, `truncation`, `return_tensors`\n",
        "- **Q1.2:** For the above arguments, explain what are the potential advantages of setting them to the default values we provide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypEhAyqLdfuK"
      },
      "source": [
        "# Step 2: Loading Data\n",
        "Here, you will load the data using the Dataloader of the dataset you built from the previous step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snh8dTy2d88M"
      },
      "source": [
        "## **Coding Exercises** for Step 2:\n",
        "You will complete the following code blocks denoted by `TODO:`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW5cX6L6SuNt",
        "outputId": "a4846811-3d83-46dc-dadf-b7ed9000bed4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7e8d9bb0c9742ce9ca520e7d5347200",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1691d1ad3b640dd9542426794b6df32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SST2Example(text=\"It 's a lovely film with lovely performances by Buy and Accorsi .\", label=0)\n",
            "6920\n",
            "872\n",
            "1821\n",
            "{'text_encoding': {'input_ids': tensor([[    0,   243,   128,  ...,     1,     1,     1],\n",
            "        [    0,  2409,   114,  ...,     1,     1,     1],\n",
            "        [    0,   250,  3279,  ...,     1,     1,     1],\n",
            "        ...,\n",
            "        [    0,   250, 10727,  ...,     1,     1,     1],\n",
            "        [    0,   713, 19907,  ...,     1,     1,     1],\n",
            "        [    0,   250, 16044,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}, 'label_encoding': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 45])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 45])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Load train / validation / test dataset, using `initialize_datasets` in `dataset.py`.\n",
        "\"\"\"\n",
        "# TODO: load pre-trained tokenizer for Roberta-base from transformers library.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# TODO: load datasets using initialize_datasets.\n",
        "datasets = initialize_datasets(tokenizer)\n",
        "dataset_train, dataset_dev, dataset_test = datasets[\"train\"], datasets[\"validation\"], datasets[\"test\"]\n",
        "\n",
        "# TODO: get the first data point in your validation dataset.\n",
        "# Hint: (for you to debug) you returned data point should look like `SST2Example(text=\"It 's a lovely ...\", label=0)`\n",
        "val_first_element = next(iter(dataset_dev))\n",
        "print(val_first_element)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: get the length of train, validation, and test datasets using `datasets` variable.\n",
        "length_train = len(dataset_train)\n",
        "length_val = len(dataset_dev)\n",
        "length_test = len(dataset_test)\n",
        "\n",
        "print(length_train)\n",
        "print(length_val)\n",
        "print(length_test)\n",
        "\n",
        "\"\"\"\n",
        "To load batch of samples from `torch.Dataset` during training / inference, we use `DataLoader` class.\n",
        "Below, we provide an example of loading a dataloader for the validation split of SST-2.\n",
        "\"\"\"\n",
        "validation_dataloader = DataLoader(datasets['validation'],\n",
        "                                   batch_size=64,\n",
        "                                   shuffle=False,\n",
        "                                   collate_fn=SST2Dataset.collate_fn)\n",
        "\n",
        "# TODO: load the first batch of samples from the validation dataset\n",
        "# Hint: use iterator!\n",
        "batch = next(iter(validation_dataloader))\n",
        "print(batch)\n",
        "\n",
        "input_ids = batch[\"text_encoding\"][\"input_ids\"]\n",
        "attention_mask = batch[\"text_encoding\"][\"attention_mask\"]\n",
        "print(type(input_ids))\n",
        "print(input_ids.shape)\n",
        "print(type(attention_mask))\n",
        "print(attention_mask.shape)\n",
        "print(type(batch[\"label_encoding\"]))\n",
        "print(batch[\"label_encoding\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdK1Wx82Uyut"
      },
      "source": [
        "## **Questions to Answer** for Step 2:\n",
        "\n",
        "**Answer these questions in your write-up report.**\n",
        "\n",
        "- **Q2.1:** What are the lengths of train, validation, test datasets?\n",
        "\n",
        "- **Q2.2:** Explain the role of each of the following parameters `batch_size`, `shuffle`, `collate_fn`, `num_workers` given to the `DataLoader` in the above code block. (Hint: You can refer to the Pytorch tutorial on Data processing in the [official website](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).)\n",
        "\n",
        "- **Q2.3:** Write the *type* and *shape* (if the type is tensor) of `input_ids`, `attention_mask`, and `label_encoding` in `batch` and explain *what do these elements represent*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EwftWQpXJ-4"
      },
      "source": [
        "# Step 3: Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kZirwxUXYd6e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Optimizer, AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fwo2RQsoeP0"
      },
      "source": [
        "## **Coding Exercises** for Step 3:\n",
        "Here, we provide a skeleton for two functions, `train_one_epoch` and `evaluate`. **You will complete code blocks denoted by `TODO:`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o3d2qrp2XPqZ"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model: nn.Module, dataloader: DataLoader, optimizer: Optimizer, epoch: int):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch.\n",
        "    :param model: A pre-trained model loaded from transformers. (e.g., RobertaForSequenceClassification https://huggingface.co/docs/transformers/v4.37.0/en/model_doc/roberta#transformers.RobertaForSequenceClassification)\n",
        "    :param dataloader: A train set dataloader for SST2Dataset.\n",
        "    :param optimizer: An instance of Pytorch optimizer. (e.g., AdamW https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)\n",
        "    :param epoch: An integer denoting current epoch.\n",
        "    Trains model for one epoch.\n",
        "    \"\"\"\n",
        "    # TODO: set the model to the training mode.\n",
        "    model.train()\n",
        "    \n",
        "    with tqdm(dataloader, desc=f\"Train Ep {epoch}\", total=len(dataloader)) as tq:\n",
        "        for batch in tq:\n",
        "            # TODO: retrieve the data from your batch and send it to GPU.\n",
        "            # Hint: model.device should point to 'cuda' as you set it as such in the main function below.\n",
        "            text_encoding = batch[\"text_encoding\"].to(model.device)\n",
        "            label_encoding = batch[\"label_encoding\"].to(model.device)\n",
        "            # TODO: Compute loss by running model with text_encoding and label_encoding.\n",
        "            loss = model(input_ids=text_encoding[\"input_ids\"],\n",
        "                         attention_mask=text_encoding[\"attention_mask\"],\n",
        "                         token_type_ids=text_encoding[\"token_type_ids\"],\n",
        "                         labels=label_encoding).loss\n",
        "\n",
        "            # TODO: compute gradients and update parameters using optimizer.\n",
        "            # Hint: you need three lines of code here!\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tq.set_postfix({\"loss\": loss.detach().item()}) # for printing better-looking progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i2xd5kiSXJVu"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module, dataloader: DataLoader) -> float:\n",
        "    \"\"\"\n",
        "    Evaluate model on the dataloader and compute the accuracy.\n",
        "    :param model: A language model loaded from transformers. (e.g., RobertaForSequenceClassification https://huggingface.co/docs/transformers/v4.37.0/en/model_doc/roberta#transformers.RobertaForSequenceClassification)\n",
        "    :param dataloader: A validation / test set dataloader for SST2Dataset\n",
        "    :return: A floating number representing the accuracy of model in the given dataset.\n",
        "    \"\"\"\n",
        "    # TODO: set the model to the evaluation mode.\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with tqdm(dataloader, desc=f\"Eval\", total=len(dataloader)) as tq:\n",
        "        for batch in tq:\n",
        "            with torch.no_grad():\n",
        "                # TODO: retrieve the data from your batch and send it to GPU.\n",
        "                # Hint: model.device should point to 'cuda' as you set it as such in the main function below.\n",
        "                text_encoding = batch[\"text_encoding\"].to(model.device)\n",
        "                label_encoding = batch[\"label_encoding\"].to(model.device)\n",
        "                # TODO: inference with model and compute logits.\n",
        "                logits =  model(input_ids=text_encoding[\"input_ids\"],\n",
        "                                attention_mask=text_encoding[\"attention_mask\"],\n",
        "                                token_type_ids=text_encoding[\"token_type_ids\"],\n",
        "                                labels=label_encoding).logits # Hint: logit should be of size (batch_size, 2)\n",
        "\n",
        "                # TODO: compute list of predictions and list of labels for the current batch\n",
        "                predictions = torch.argmax(logits, dim=1) # Hint: should be a list [0, 1, ...] of predicted labels\n",
        "                labels =  label_encoding # Hint: should be a list [0, 1, ...] of ground-truth labels\n",
        "\n",
        "                all_predictions += predictions\n",
        "                all_labels += labels\n",
        "\n",
        "    # compute accuracy\n",
        "    all_predictions = torch.Tensor(all_predictions)\n",
        "    all_labels = torch.Tensor(all_labels)\n",
        "    accuracy = compute_accuracy(all_predictions, all_labels)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions: torch.Tensor, labels: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Given two tensors predictions and labels, compute the accuracy.\n",
        "    :param predictions: torch.Tensor of size (N,)\n",
        "    :param labels: torch.Tensor of size (N,)\n",
        "    :return: A floating number representing the accuracy\n",
        "    \"\"\"\n",
        "    assert predictions.size(-1) == labels.size(-1)\n",
        "\n",
        "    # TODO: compute accuracy\n",
        "    accuracy = torch.sum(predictions == labels) / len(predictions)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zhIKsk-zva0"
      },
      "source": [
        "## **Questions to Answer** for Step 3:\n",
        "**Answer these questions in your write-up report.**\n",
        "- **Q3.1:** For the three lines of code you implemented for computing gradients and updating parameters using optimizer, explain what each of the lines does, respectively.\n",
        "\n",
        "- **Q3.2:** Explain what setting the model to training and evaluation modes do, respectively.\n",
        "\n",
        "- **Q3.3:** Explain what `with torch.no_grad()` does in the `evaluation()` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c64zTgzaKNR"
      },
      "source": [
        "# Step 4: Main Training Loop\n",
        "\n",
        "The main function uses all the functions implemented above to learn a model for certain number of epochs and evaluate it. Read through the comments and implement `main` that fine-tunes the RoBERTa-based on SST-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biwuzhhYO57P"
      },
      "source": [
        "## **Coding Exercises** for Step 4:\n",
        "Here, we provide a skeleton for the `main` function. **You will complete code blocks denoted by `TODO:`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DTG5EnW7UyPL"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(64)\n",
        "\n",
        "def main():\n",
        "    # hyper-parameters (we provide initial set of values here, but you can modify them.)\n",
        "    batch_size = 64\n",
        "    learning_rate = 5e-5\n",
        "    num_epochs = 10\n",
        "    model_name = \"roberta-base\"\n",
        "\n",
        "    # TODO: load pre-trained model and corresponding tokenizer (given model_name above).\n",
        "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "    # load model on GPU.\n",
        "    model = model.to(\"cuda\")\n",
        "\n",
        "    # TODO: initialize the AdamW optimizer with optional arguments: lr=learning_rate, eps=1e-8\n",
        "    optimizer = AdamW(params=model.parameters(), lr=learning_rate, eps=1e-8)\n",
        "\n",
        "    # TODO: load datasets.\n",
        "    datasets = initialize_datasets(tokenizer)\n",
        "\n",
        "    # TODO: initialize that training and evaluation (validation / test) dataloaders.\n",
        "    # Hint: you should use the validation dataset during hyperparameter tuning,\n",
        "    # and evaluate the model on the test set once after you finalize the design choice of your model.\n",
        "    # Hint: you should shuffle the training data, but not the validation data.\n",
        "    train_dataloader = DataLoader(dataset=datasets[\"train\"],\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=False,\n",
        "                                  collate_fn=SST2Dataset.collate_fn)\n",
        "    validation_dataloader = DataLoader(dataset=datasets[\"validation\"],\n",
        "                                       batch_size=batch_size,\n",
        "                                       shuffle=False,\n",
        "                                       collate_fn=SST2Dataset.collate_fn)\n",
        "\n",
        "    # training loop.\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_one_epoch(model, train_dataloader, optimizer, epoch)\n",
        "        valid_acc = evaluate(model, validation_dataloader)\n",
        "        \n",
        "        # TODO: if the newly trained model checkpoint is better than the previously\n",
        "        if valid_acc > best_acc:\n",
        "            best_acc = valid_acc\n",
        "            torch.save({\n",
        "                \"model\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "                \"valid_acc\": valid_acc\n",
        "            }, \"./checkpoints/ckpt_best\")\n",
        "        # saved checkpoint, save the new model in `./checkpoints` folder.\n",
        "        # Hint: remember to update best_acc to the accuracy of the best model so far.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XmMc07CfUtJo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d24a9fc3f1844a39f46dd7683dcae25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Train Ep 1:   0%|          | 0/109 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Run the main training loop.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# NOTE: if implemented well, each training epoch will take less than 2 minutes.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m main()\n",
            "\u001b[1;32m/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m best_acc \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     train_one_epoch(model, train_dataloader, optimizer, epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     valid_acc \u001b[39m=\u001b[39m evaluate(model, validation_dataloader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m# TODO: if the newly trained model checkpoint is better than the previously\u001b[39;00m\n",
            "\u001b[1;32m/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m label_encoding \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel_encoding\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# TODO: Compute loss by running model with text_encoding and label_encoding.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49mtext_encoding[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m              attention_mask\u001b[39m=\u001b[39;49mtext_encoding[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m              token_type_ids\u001b[39m=\u001b[39;49mtext_encoding[\u001b[39m\"\u001b[39;49m\u001b[39mtoken_type_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m              labels\u001b[39m=\u001b[39;49mlabel_encoding)\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# TODO: compute gradients and update parameters using optimizer.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Hint: you need three lines of code here!\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anderson/Documents/CSE/cse447-nlp/hw2/A2S3.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1198\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1198\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1199\u001b[0m     input_ids,\n\u001b[1;32m   1200\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1201\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1202\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1203\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1204\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1205\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1206\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1207\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1208\u001b[0m )\n\u001b[1;32m   1209\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    828\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    829\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    830\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    834\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    836\u001b[0m     embedding_output,\n\u001b[1;32m    837\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    838\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    839\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    840\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    841\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    842\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    843\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    844\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    845\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    514\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    525\u001b[0m         hidden_states,\n\u001b[1;32m    526\u001b[0m         attention_mask,\n\u001b[1;32m    527\u001b[0m         layer_head_mask,\n\u001b[1;32m    528\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    529\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    530\u001b[0m         past_key_value,\n\u001b[1;32m    531\u001b[0m         output_attentions,\n\u001b[1;32m    532\u001b[0m     )\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    414\u001b[0m         hidden_states,\n\u001b[1;32m    415\u001b[0m         attention_mask,\n\u001b[1;32m    416\u001b[0m         head_mask,\n\u001b[1;32m    417\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    418\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    419\u001b[0m     )\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 340\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    341\u001b[0m         hidden_states,\n\u001b[1;32m    342\u001b[0m         attention_mask,\n\u001b[1;32m    343\u001b[0m         head_mask,\n\u001b[1;32m    344\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    345\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    346\u001b[0m         past_key_value,\n\u001b[1;32m    347\u001b[0m         output_attentions,\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    350\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:266\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    263\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n\u001b[1;32m    265\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(attention_scores, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    268\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_probs)\n",
            "File \u001b[0;32m~/Documents/CSE/cse447-nlp/.conda/lib/python3.10/site-packages/torch/nn/functional.py:1828\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1824\u001b[0m         ret \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39m\u001b[39minput\u001b[39m)\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m   1825\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n\u001b[0;32m-> 1828\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax\u001b[39m(\u001b[39minput\u001b[39m: Tensor, dim: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, _stacklevel: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, dtype: Optional[DType] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m   1829\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Apply a softmax function.\u001b[39;00m\n\u001b[1;32m   1830\u001b[0m \n\u001b[1;32m   1831\u001b[0m \u001b[39m    Softmax is defined as:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1851\u001b[0m \n\u001b[1;32m   1852\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Run the main training loop.\n",
        "# NOTE: if implemented well, each training epoch will take less than 2 minutes.\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fam6lLvAu9r"
      },
      "source": [
        "## **Questions to Answer** for Step 4:\n",
        "**Answer these questions in your write-up report.**\n",
        "- **Q4.1:** With the following default hyperparameters we provide, plot both training and validation loss curves across 10 epochs in a single plot (x-axis: num of the epoch; y-axis: acc). You can draw this plot with a Python script or other visualization tools like Google Sheets.\n",
        "\n",
        "  - batch_size = 64\n",
        "  - learning_rate = 5e-5\n",
        "  - num_epochs = 20\n",
        "  - model_name = \"roberta-base\"\n",
        "\n",
        "- **Q4.2:** Describe the behaviors of the training and validation loss curves as you plotted above. At which epoch does the model achieve the best accuracy on the training dataset? What about the validation dataset? Do training and validation curves have the same trend? Why does the current trend happen?\n",
        "\n",
        "- **Q4.3:** Why do you shuffle the training data but not the validation data?\n",
        "\n",
        "- **Q4.4:** Explain the functionality of optimizers.\n",
        "\n",
        "- **Q4.5:** Experiment with two other optimizers defined in `torch.optim` for the training the model with the default hyperparameters we give you. What's the difference between `AdamW` and these two new optimizers? Back up your claims with emperical evidence.\n",
        "\n",
        "- **Q4.6:** Experiment with different combinations of `batch_size`, `learning_rate`, and `num_epochs`. Your goal is to pick the final, best model checkpoint based on the validation dataset accuracy. Describe the strategy you used to try different combinations of hyperparameters. Why did you use this strategy?\n",
        "\n",
        "- **Q4.7:** What are the `batch_size`, `learning_rate`, and `num_epochs` of the best model checkpoint that you picked? What are the training accuracy and validation accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAJarNtCWoVi"
      },
      "source": [
        "# Step 5: Testing the Final Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY34O9hqcyAD"
      },
      "source": [
        "## **Coding Exercises** for Step 5:\n",
        "Here, you load your best trained model from `./checkpoints/` and report the test set accuracy. **You will complete the following code blocks denoted by `TODO:`.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmy87rriepxY"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# TODO: Load your best trained model from `./checkpoints/` and report the test set accuracy.\n",
        "model =\n",
        "\n",
        "datasets = initialize_datasets(tokenizer)\n",
        "# TODO: Load the test dataset\n",
        "test_dataloader =\n",
        "\n",
        "# TODO: evaluate the model on the test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXoW_6jEM91z"
      },
      "source": [
        "## **Questions to Answer** for Step 5:\n",
        "**Answer these questions in your write-up report.**\n",
        "- **Q5.1:** What's the test set accuracy of the best model?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
